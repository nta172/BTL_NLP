{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>1. Đọc file ngữ liệu đã thu thập</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số câu dữ liệu:  812\n"
     ]
    }
   ],
   "source": [
    "f = open('data/corpus.txt', 'r', encoding= 'utf-8')\n",
    "data = [line[:-1] for line in f.readlines()]\n",
    "f.close()\n",
    "\n",
    "print('Số câu dữ liệu: ', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Khi xã hội chưa thể là \"ông chủ trả lương đúng và đủ\" cho giáo viên, thì sự tôn trọng và thấu hiểu sẽ là nguồn động viên to lớn để thầy cô giáo làm tốt công việc của mình mỗi ngày.',\n",
       " 'Tuy nhiên, đây mới là lần đầu siêu sao 39 tuổi chơi bóng tại Scotland, và anh tạo ra cơn sốt với khán giả ở đó.',\n",
       " 'Đồng thời, người sản xuất túi mù cũng đang tận dụng sức hút của Labubu. Ở Việt Nam, dữ liệu từ Metric - nền tảng thống kê thương mại điện tử - cho biết, vào quý II, các mặt hàng liên quan Labubu (chính hãng, xách tay) mang về gần 5,2 tỷ đồng trên Shopee, Lazada, Tiktok Shop.',\n",
       " 'Theo đề xuất của Bộ Giao thông Vận tải, dự án đường sắt tốc độ cao Bắc Nam có tổng mức đầu tư sơ bộ hơn 1,7 triệu tỷ đồng (tương đương 67,34 tỷ USD), tuyến đường đôi, khổ 1.',\n",
       " 'Nhiếp ảnh gia Lê Bích 52 tuổi, quê Hà Nội.',\n",
       " '000 đồng mỗi kg.',\n",
       " 'Phong cũng là người thích \"túi mù\".',\n",
       " 'Trong đó, tầng một có hai cầu vượt: cầu số 1 gồm hai nhánh (6 làn xe) nằm trên Vành đai 2, vượt qua đường Linh Đông, Phạm Văn Đồng, Kha Vạn Cân, rạch Ngang; cầu số 2 có hai làn xe, rẽ trái từ Vành đai 2 xuống đường Phạm Văn Đồng, hướng về sân bay Tân Sơn Nhất.',\n",
       " '\"Thấy miếng xốp trôi dạt, tôi vớ lấy, nhét vào áo trước bụng rồi cố nổi lên\", anh kể.',\n",
       " '000 đồng, tùy theo kích cỡ.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>2. Tạo danh sách chứa đầy đủ các ký tự trong tiếng Việt và các danh sach chứa những ký tự thường lỗi hoặc từ viết</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel = list('aàáảãạăằắẳẵặâầấẩẫậeèéẻẽẹêềếểễệiìíỉĩịoòóỏõọôồốổỗộơờớởỡợuùúủũụưừứửữựyỳýỷỹỵAÀÁẢÃẠĂẰẮẲẴẶÂẦẤẨẪẬEÈÉẺẼẸÊỀẾỂỄỆIÌÍỈĨỊOÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢUÙÚỦŨỤƯỪỨỬỮỰYỲÝỶỸỴ')\n",
    "full_letters = vowel + list('bcdfghjklmnpqrstvmxzBCDFGHJKLMNPQRSTVWXZdĐ')\n",
    "\n",
    "# Những lỗi thường gặp trong Tiếng Việt\n",
    "typo = {\n",
    "    # Chữ thường\n",
    "    'á':'as','à':'af','ả':'ar','ã':'ax','ạ':'aj',\n",
    "    'é':'es','è':'ef','ẻ':'er','ẽ':'ex','ẹ':'ej',\n",
    "    'í':'is','ì':'if','ỉ':'ir','ĩ':'ix','ị':'ij',\n",
    "    'ó':'os','ò':'of','ỏ':'or','õ':'ox','ọ':'oj',\n",
    "    'ú':'us','ù':'uf','ủ':'ur','ũ':'ux','ụ':'uj',\n",
    "    'ý':'ys','ỳ':'yf','ỷ':'yr','ỹ':'yx','ỵ':'yj',\n",
    "\n",
    "    # Chữ có dấu mũ\n",
    "    'ấ':'aas','ầ':'aaf','ẩ':'aar','ẫ':'aax','ậ':'aaj',\n",
    "    'ế':'ees','ề':'eef','ể':'eer','ễ':'eex','ệ':'eej',\n",
    "    'ố':'oos','ồ':'oof','ổ':'oor','ỗ':'oox','ộ':'ooj',\n",
    "\n",
    "    # Chữ có dấu móc (ơ, ư)\n",
    "    'ớ':'ows','ờ':'owf','ở':'owr','ỡ':'owx','ợ':'owj',\n",
    "    'ứ':'uws','ừ':'uwf','ử':'uwr','ữ':'uwx','ự':'uwj',\n",
    "\n",
    "    # Chữ in hoa\n",
    "    'Á':'As','À':'Af','Ả':'Ar','Ã':'Ax','Ạ':'Aj',\n",
    "    'É':'Es','È':'Ef','Ẻ':'Er','Ẽ':'Ex','Ẹ':'Ej',\n",
    "    'Í':'Is','Ì':'If','Ỉ':'Ir','Ĩ':'Ix','Ị':'Ij',\n",
    "    'Ó':'Os','Ò':'Of','Ỏ':'Or','Õ':'Ox','Ọ':'Oj',\n",
    "    'Ú':'Us','Ù':'Uf','Ủ':'Ur','Ũ':'Ux','Ụ':'Uj',\n",
    "    'Ý':'Ys','Ỳ':'Yf','Ỷ':'Yr','Ỹ':'Yx','Ỵ':'Yj',\n",
    "\n",
    "    # Chữ in hoa có dấu mũ\n",
    "    'Ấ':'Aas','Ầ':'Aaf','Ẩ':'Aar','Ẫ':'Aax','Ậ':'Aaj',\n",
    "    'Ế':'Ees','Ề':'Eef','Ể':'Eer','Ễ':'Eex','Ệ':'Eej',\n",
    "    'Ố':'Oos','Ồ':'Oof','Ổ':'Oor','Ỗ':'Oox','Ộ':'Ooj',\n",
    "\n",
    "    # Chữ in hoa có dấu móc (Ơ, Ư)\n",
    "    'Ớ':'Ows','Ờ':'Owf','Ở':'Owr','Ỡ':'Owx','Ợ':'Owj',\n",
    "    'Ứ':'Uws','Ừ':'Uwf','Ử':'Uwr','Ữ':'Uwx','Ự':'Uwj'\n",
    "}\n",
    "\n",
    "\n",
    "region = {'ả':'ã','ã':'ả','ẻ':'ẽ','ẽ':'ẻ','ỉ':'ĩ','ĩ':'ỉ','ỏ':'õ','õ':'ỏ','ủ':'ũ','ũ':'ủ','ỷ':'ỹ','ỹ':'ỷ', \n",
    "          'À':'Ã','Ã':'À','È':'Ẽ','Ẽ':'È','Ì':'Ĩ','Ĩ':'Ì','Ò':'Õ','Õ':'Ò','Ù':'Ũ','Ũ':'Ù','Ỳ':'Ỹ','Ỹ':'Ỳ'}\n",
    "\n",
    "\n",
    "region2 = {'ch':'tr', 'tr':'ch','gi':'d','d':'gi','l':'n','n':'l','s':'x','x':'s',\n",
    "           'Ch':'Tr', 'Tr':'Ch','Gi':'D','D':'Gi','L':'N','N':'L','S':'X','X':'S'}\n",
    "\n",
    "acronym = {'anh':'a','biết':'bít','chồng':'ck','được':'đc','em':'e','gì':'j','giờ':'h',\n",
    "           'Anh':'A','Biết':'Bít','Chồng':'Ck','Được':'Đc','Em':'E','Gì':'J','Giờ':'H',\n",
    "           'không':'ko','muốn':'mún','ông':'ôg','phải':'fai','tôi':'t','vợ':'vk','yêu':'iu',\n",
    "           'Không':'Ko','Muốn':'Mún','Ông':'Ôg','Phải':'Fai','Tôi':'T','Vợ':'Vk','Yêu':'Iu',\n",
    "           'bạn':'b','rồi':'r','Bạn':'B','Rồi':'r'\n",
    "           } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>3. Viết hàm tạo ra dữ liệu nhiễu</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Hàm thêm từ viết tắt\n",
    "def _teen_code(sentence):\n",
    "    random = np.random.uniform(0,1,1)[0]\n",
    "    new_sentence = str(sentence)\n",
    "\n",
    "    if random > 0.5:\n",
    "        for word in acronym.keys():\n",
    "            if re.search(word, new_sentence):\n",
    "                random2 = np.random.uniform(0,1,1)[0]\n",
    "                if random2 < 0.5:\n",
    "                    new_sentence = re.sub(word, acronym[word], new_sentence)\n",
    "        return new_sentence\n",
    "    else:\n",
    "        return sentence\n",
    "# Hàm thêm các lỗi\n",
    "def _add_noise(sentence):\n",
    "    sentence = _teen_code(sentence)\n",
    "    noisy_sentence = ''\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        if sentence[i] not in full_letters:\n",
    "            noisy_sentence += sentence[i]\n",
    "        else:\n",
    "            random = np.random.uniform(0,1,1)[0]\n",
    "            if random <= 0.94:\n",
    "                noisy_sentence += sentence[i]\n",
    "            elif random <= 0.985:\n",
    "                if sentence[i] in typo.keys():\n",
    "                    if sentence[i] in region.keys():\n",
    "                        random2 = np.random.uniform(0,1,1)[0]\n",
    "                        if random2 <= 0.4:\n",
    "                            noisy_sentence += typo[sentence[i]]\n",
    "                        elif random2 < 0.8:\n",
    "                            noisy_sentence += region[sentence[i]]\n",
    "                        elif random < 0.95:\n",
    "                            noisy_sentence += unidecode(sentence[i])\n",
    "                        else:\n",
    "                            noisy_sentence += sentence[i]\n",
    "                    else:\n",
    "                        random3 = np.random.uniform(0,1,1)[0]\n",
    "                        if random3 <= 0.6:\n",
    "                            noisy_sentence += typo[sentence[i]]\n",
    "                        elif random3 < 0.9:\n",
    "                            noisy_sentence += unidecode(sentence[i])\n",
    "                        else:\n",
    "                            noisy_sentence += sentence[i]\n",
    "                elif i == 0 or sentence[i-1] not in full_letters:\n",
    "                    random4 = np.random.uniform(0,1,1)[0]\n",
    "                    if random4 <= 0.9:\n",
    "                        if i < len(sentence) - 1 and sentence[i] in region2.keys() and sentence[i+1] in vowel:\n",
    "                            noisy_sentence += region2[sentence[i]]\n",
    "                        elif i < len(sentence) - 2 and sentence[i:i+2] in region2.keys() and sentence[i+2] in vowel:\n",
    "                            noisy_sentence += region2[sentence[i:i+2]]\n",
    "                            i += 1\n",
    "                        else:\n",
    "                            noisy_sentence += sentence[i]\n",
    "                    else:\n",
    "                        noisy_sentence += sentence[i]\n",
    "                else:\n",
    "                    noisy_sentence +- sentence[i]\n",
    "            else:\n",
    "                new_random = np.random.uniform(0,1,1)[0]\n",
    "                if new_random <= 0.33 and i != len(sentence) - 1:\n",
    "                    noisy_sentence += sentence[i+1]\n",
    "                    noisy_sentence += sentence[i]\n",
    "                    i += 1\n",
    "                else:\n",
    "                    noisy_sentence += sentence[i]\n",
    "        i += 1\n",
    "    return noisy_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T tên là Nguyễn Thế A'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_add_noise('Tôi tên là Nguyễn Thế Anh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "['\\x00', ' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'à', 'á', 'ả', 'ã', 'ạ', 'ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'A', 'À', 'Á', 'Ả', 'Ã', 'Ạ', 'Ă', 'Ằ', 'Ắ', 'Ẳ', 'Ẵ', 'Ặ', 'Â', 'Ầ', 'Ấ', 'Ẩ', 'Ẫ', 'Ậ', 'E', 'È', 'É', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ề', 'Ế', 'Ể', 'Ễ', 'Ệ', 'I', 'Ì', 'Í', 'Ỉ', 'Ĩ', 'Ị', 'O', 'Ò', 'Ó', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ồ', 'Ố', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ờ', 'Ớ', 'Ở', 'Ỡ', 'Ợ', 'U', 'Ù', 'Ú', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ừ', 'Ứ', 'Ử', 'Ữ', 'Ự', 'Y', 'Ỳ', 'Ý', 'Ỷ', 'Ỹ', 'Ỵ', 'b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'm', 'x', 'z', 'B', 'C', 'D', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'X', 'Z', 'd', 'Đ']\n"
     ]
    }
   ],
   "source": [
    "# Các ký tự có thể xuất hiện trong encoder\n",
    "alphabet = ['\\x00', ' '] + list('0123456789') + full_letters\n",
    "print(len(alphabet))\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
